id,title
31643,"AutoTokenizer: Phi-3 drops spaces when decodes a token at a time"
31642,"return_dict in encodec is always set to True: "
31641,"Using batching with pipeline and transformers"
31640,"change anchor_image_size None for compatibility"
31636,"No module named 'transformers.models.starcoder2'"
31635,"Please reopen issue #30361"
31633,"`convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.py` does not work due to weight norm key"
31630,"The behavior of the tokenizer loaded from GGUF file is incorrect."
31629,"Enhancing SFT Training Efficiency Using Packing and FlashAttention2 with Position IDs"
31628,"Fix runtime error for devices with MPS support"
31627,"Tokenizer discard data that exceed max_length"
31626,"New `save_strategy` option called ""best"" to save when a new best performance is achieved."
31625,"enable low-precision pipeline"
31624,"When max_steps < save_steps with deepspeed zero3 stage"
31623,"Added HHCache class implementing H2O Cache"
31622,"Unable to export Phi-3-vision model to PyTorch exported program"
31592,"Added Turkish README"
31591,"[`Llama`] Conversion: fix and simplify the script!"
31590,"[QoL] Allow dtype str for torch_dtype arg of from_pretrained"
31588,"Incorrect docstring of `get_anyres_image_grid_shape`"
31585,"[Bug Fix] fix qa pipeline tensor to numpy"
31583,"`pip install accelerate` (and similar) error messages should specify min version"
31582,"Multi-GPU inference affects LLM's (Llama2-7b-chat-hf) generation."
31580,"Allow infer_framework_load_model to use the originally specified config."
31579,"[WIP] Add implementation of `_extract_fbank_features_batch` "
31578,"add warning when using gradient_checkpointing with FSDP full shard"
31577,"OOM when loading 300B models with `AutoModelForCausalLM.from_pretrained` and `BitsAndBytesConfig` quantization."
31574,"transformers.fx.symbolic_trace supports inputs_embeds"
31572,"Add language to word timestamps for Whisper"
31571,"RuntimeError: slow_conv2d_forward_mps: input(device='cpu') and weight(device=mps:0') "
31570,"Trainer: To keep unused columns for `compute_metrics`"
31567,"rewoo plan, critique"
31564,"Optimize 1st token for beam_search"
31561,"Add argument to set number of eval steps in Trainer"
31560,"NotImplementedError: Cannot copy out of meta tensor; no data when embedding to meta"
31557,"You can't train a model that has been loaded with `device_map='auto'` in any distributed mode."
31556,"Fixing Tensor Shape/Dimension Mismatch Errors in TimeSeries Transformer for Stock Price Prediction"
31554,"fix HF conversion for older Llama versions"
31552,"Fail to load model without .safetensors file"
31551,"Index out of range when generate using optimum"
31550,"Test loading generation config with safetensor weights"
31549,"Mamba & RecurrentGemma: enable strict signature"
31546,"Add torch_empty_cache_steps to TrainingArguments"
31545,"Problem with the masked language modeling tutorial "
31544,"Nested from_pretrained() gives warnings loading weights - ""copying from a non-meta parameter"""
31541,"handle when from_pretrained_id is a list"
31540,"Batch Generation giving different output when using batch size > 1 or when using padding in MambaForCausalLM"
31538,"fix wav2vec2 with torch.compile"
31537,"Sink cache: fix generate from cache"
31534,"Chameleon: add model"
31533,"HuggingFace GroundingDINO inference execution time is slower than the original groundingDINO (~100ms)"
31531,"Source link to `LlamaForSequenceClassification` seems broken, if so, update it."
31529,"Potential Bug in llava_next when calling pack_image_features function."
31528,"Error on fine tuning paligemma for object detection"
31527,"GGML (GGUF) Llama3 unit test fails"
31525,"Support deepspeed sequence parallel"
31522,"Depth Anything: update conversion script for V2"
31521,"Sequence Length Invariant Text Models"
31520,"Fix mininal version check for object_detection.md"
31518,"Cohere: Use `diff` instead of `Copied from` mechanism"
31515,"from_pretrained 加载checkpoint过慢的问题"
31514,"add gather_use_object arguments"
31512,"add bnb support for Ascend NPU"
31509,"Check diff files in `check_copies`"
31508,"如果在单个GPU上out of memory 如何用两个GPU加载推理同一个模型？"
31507,"load qwen2-72b-instruct sft awq q4_0 gguf ValueError: Trying to set a tensor of shape torch.Size"
31506,"Add Florence2 support"
31505,"Meta FAIR Chameleon 7b and 30b"
31504,"RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1! (HF/Accelerate)"
31502,"Granite language models [WIP]"
31501,"FineWeb SLM Training doesn't start"
31500,"MixtralFlashAttention2: put ""plus 1"" inside parentheses when calculating rotary_seq_len, allowing None position_ids input."
31499,"Add FA2 and `sdpa` support for SigLIP"
31498,"RecurrentGemma: add generative tests"
31497,"will this be a model or just a tokenizer/embeddings ? technically a sentence trancformer ?"
31495,"Add training support for SigLIP"
31494,"[WIP] - Add Descript-Audio-Codec model"
31492,"ImportError: cannot import name 'logging' from 'huggingface_hub'"
31487,"run_clm.py AttributeError: 'NoneType' object has no attribute 'get'"
31486,"The last ut test of the QDQBert model ”test_inference_no_head_absolute_embedding” did not pass when using official safetensors"
31485,"ChatGLMForConditionalGeneration does not support Flash Attention 2.0 yet."
31484,"GenerationMixin sample() runs forever"
31483,"Add French version of run scripts tutorial"
31482,"Update beam_constraints with KMP"
31481,"Deprecate models - June 2024"
31479,"Quantized T5EncoderModel cannot be removed from VRAM on CUDA systems"
31475,"How do I replace a spare tokens?"
31474,"Quantization support for heads and embeddings"
31469,"FIX / Hub: Also catch for `exceptions.ConnectionError`"
31468,"Attention dropout causing problem in attention score distribution"
31467,"Gemma-7b model set my own lm_head but cannot saved and changed pretrained embedding_layer's weights too."
31464,"Mixtral's implementation of auxiliary loss seems incorrect"
31463,"add changes in mistral model to avoid problems in pytorch hooks"
31462,"A parameter in TrainingArguments: sample_output=True"
31461,"linear_sum_assignment error in the object_detection.py guide"
31460,"cannot import name 'AutoModelForImageToImage' from 'transformers.models.auto.modeling_auto' (/opt/conda/lib/python3.10/site-packages/transformers/models/auto/modeling_auto.py)"
31458,"Bump scikit-learn from 1.0.2 to 1.5.0 in /examples/research_projects/decision_transformer"
31457,"Trainer having issues with DataLoaderShard when running with torchrun"
31454,"Can't create transformer pipeline because pytorch failed to be detected"
31453,"How to build and evaluate a vanilla transformer?"
